# Improve EvoSuite Testsuites Readability

## Prerequisites
1. Having generated the tests with evosuite and placed them in the folder /src/test/java/...
2. Have Jacoco set up as a maven plugin within the pom.xml of your project(s).
```xml
<build>
    <defaultGoal>...</defaultGoal>
    <pluginManagement>
        <plugins>
            <plugin>
                <groupId>org.jacoco</groupId>
                <artifactId>jacoco-maven-plugin</artifactId>
                <version>0.8.12</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>prepare-agent</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>report</id>
                        <phase>test</phase>
                        <goals>
                            <goal>report</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </pluginManagement>
</build>
```

## How to install the tool

1. clone the project via github:
> git clone "link"

2. Open the terminal and get in the root of the project.

3. If you can't see a virual environment on your terminal, create it:
> python -m venv venv

4. Activate the virtual environment:

*Windows*
> .\venv\Scripts\activate

*macOS or Linux*
> source venv/bin/activate

5. Install the requirements:
> pip install -r requirements.txt

6. Run the tool:
> streamlit run main.py


## How to use the tool

Once you have started and correctly displayed the tool's start page, you will be asked to enter inputs:

1. **Model selection**: choose the number selecting one of the model in the list.
2. **Temperature selection**: choose the temperature at which you want the task to be executed.
3. **Projects paths**: enter the complete project paths (e.g. \Users\yourusername) containing the tests automatically generated by evosuite (step to be taken before using the tool) and whose tests you want to improve.
4. **Output path**: enter the complete paths of the output folder in which the tool can save the results.
5. **Repetition**: insert a number between 1 to 10 representing the number of time you want to repeat the readability improving process.


## How to interpret the results:
Once the tool has finished its process, you will find in the output folder a folder for each analysed project and within it the results of the tool:
1. The folders numbered from *0* to a maximum of *9* contain the improved testsuites, each folder containing the results of each repetition specified by the user as input.
2. The *evosuite* folder contains the original evosuite tests, used by the tool during the improving process.
3. The *jacocoresults* folder contains the reports saved and used by the tool during the improving process.
4. The *embeddings results* folder contains the results of the comparisons of the tests embeddings in the various repetitions of the improvement process.
    > (0, 1): [0.96]
     (0, 2): [0.96]
     (1, 2): [1.0]
    
    The above is an example of the output produced by the embeddings. The numbers within the parentheses indicate the repetitions being compared, while the square brackets on the right contain the results, represented as a list of floating-point values. In this particular case, the test suite contained only a single test. However, if there were multiple tests, the output would include as many numbers as there are tests, following the order in which they appear in the test suite.
   
5. The files *comparison_results_aggregate* and *comparison_results_specific* contain the Boolean results of the Jacoco report comparisons across various iterations. These comparisons aim to assess the preservation of test semantics following the readability improvements made by the model. 

    >   0: True
        1: True
        2: True
   > 
   In the previous example, the results of three different iterations are presented. The value *0:True* indicates that the modified test in the first iteration of the improvement process, when compared to the original EvoSuite test, did not experience any change in semantics. This confirms that the model successfully modified the test identifiers without altering their behavior or semantics. Conversely, a *False* value would indicate a change in semantics.







